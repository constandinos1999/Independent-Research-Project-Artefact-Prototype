{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!\"{sys.executable}\" -m pip install --upgrade pyarrow"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import pyarrow\n",
    "import sys\n",
    "\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"pyarrow:\", pyarrow.__version__)"
   ],
   "id": "3063869c65a22cda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(\"/Users/constan/Desktop\")\n",
    "DATA_DIR = ROOT / \"data\" / \"cicids2017_raw\"\n",
    "\n",
    "parquets = sorted(DATA_DIR.glob(\"*.parquet\"))\n",
    "print(\"Parquet files found:\", len(parquets))\n",
    "\n",
    "dfs = []\n",
    "for p in parquets:\n",
    "    df = pd.read_parquet(p)\n",
    "\n",
    "    name = p.name.lower()\n",
    "    if \"benign\" in name:\n",
    "        df[\"label\"] = \"benign\"\n",
    "        df[\"attack_type\"] = \"none\"\n",
    "    else:\n",
    "        df[\"label\"] = \"classical_attack\"\n",
    "        df[\"attack_type\"] = p.stem.replace(\"-no-metadata\", \"\")\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "df_raw = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Total rows loaded:\", len(df_raw))\n",
    "df_raw.head()\n"
   ],
   "id": "24151fdbfbe88011"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- classical feature candidates (CICFlowMeter style) ---\n",
    "CLASSICAL_NUM = [\n",
    "    \"Flow Duration\",\n",
    "    \"Total Fwd Packets\", \"Total Backward Packets\",\n",
    "    \"Total Length of Fwd Packets\", \"Total Length of Bwd Packets\",\n",
    "    \"Flow Packets/s\", \"Flow Bytes/s\",\n",
    "    \"Fwd Packet Length Mean\", \"Bwd Packet Length Mean\",\n",
    "    \"Flow IAT Mean\", \"Flow IAT Std\",\n",
    "    \"SYN Flag Count\", \"FIN Flag Count\", \"RST Flag Count\",\n",
    "    \"ACK Flag Count\", \"PSH Flag Count\", \"URG Flag Count\",\n",
    "]\n",
    "CLASSICAL_CAT = [\"Protocol\"]\n",
    "\n",
    "keep = [c for c in CLASSICAL_CAT + CLASSICAL_NUM if c in df_raw.columns]\n",
    "cat_cols = [c for c in CLASSICAL_CAT if c in keep]\n",
    "\n",
    "print(\"Classical columns used:\", keep)\n",
    "\n",
    "df = df_raw[keep + [\"label\", \"attack_type\"]].copy()\n",
    "\n",
    "# Clean numeric columns\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "for c in keep:\n",
    "    if c in cat_cols:\n",
    "        continue\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "df.head()\n"
   ],
   "id": "46e19e344e193ca6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RNG = np.random.default_rng(42)\n",
    "QUANTUM_ATTACKS = np.array([\n",
    "    \"detector_blinding\",\n",
    "    \"time_shift\",\n",
    "    \"pns\",\n",
    "    \"lo_manipulation\"\n",
    "])\n",
    "\n",
    "# ---------- QKD benign telemetry ----------\n",
    "def qkd_benign(n):\n",
    "    qber = RNG.beta(2.0, 18.0, size=n) * 0.04 + 0.005\n",
    "    clicks = RNG.lognormal(mean=6.0, sigma=0.25, size=n)\n",
    "    d0 = np.clip(clicks * RNG.normal(0.5, 0.03, size=n), 0, None)\n",
    "    d1 = np.clip(clicks - d0, 0, None)\n",
    "    imb = np.abs(d0 - d1) / np.maximum(1.0, d0 + d1)\n",
    "\n",
    "    decoy = np.clip(RNG.normal(0.30, 0.05, size=n), 0.05, 0.60)\n",
    "    loss_db = np.clip(RNG.normal(3.0, 1.0, size=n), 0.0, 15.0)\n",
    "    power = np.clip(RNG.normal(1.0, 0.08, size=n), 0.6, 1.4)\n",
    "    noise = np.clip(RNG.normal(0.0, 0.15, size=n), -0.5, 1.5)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"qber\": qber,\n",
    "        \"click_d0\": d0,\n",
    "        \"click_d1\": d1,\n",
    "        \"detector_imbalance\": imb,\n",
    "        \"decoy_signal_ratio\": decoy,\n",
    "        \"channel_loss_db\": loss_db,\n",
    "        \"optical_power_rel\": power,\n",
    "        \"excess_noise\": noise,\n",
    "    })\n",
    "\n",
    "QCOLS = [\n",
    "    \"qber\",\n",
    "    \"click_d0\",\n",
    "    \"click_d1\",\n",
    "    \"detector_imbalance\",\n",
    "    \"decoy_signal_ratio\",\n",
    "    \"channel_loss_db\",\n",
    "    \"optical_power_rel\",\n",
    "    \"excess_noise\",\n",
    "]\n",
    "\n",
    "# ---------- Quantum attack perturbations ----------\n",
    "def apply_quantum_attack(qkd_df, kinds):\n",
    "    out = qkd_df.copy()\n",
    "    kinds = np.asarray(kinds)\n",
    "\n",
    "    # detector blinding\n",
    "    idx = np.where(kinds == \"detector_blinding\")[0]\n",
    "    if len(idx):\n",
    "        out.iloc[idx, out.columns.get_loc(\"optical_power_rel\")] = np.clip(\n",
    "            RNG.normal(1.55, 0.12, len(idx)), 1.3, 2.2\n",
    "        )\n",
    "        out.iloc[idx, out.columns.get_loc(\"qber\")] = np.clip(\n",
    "            RNG.normal(0.035, 0.01, len(idx)), 0.01, 0.07\n",
    "        )\n",
    "\n",
    "    # time-shift\n",
    "    idx = np.where(kinds == \"time_shift\")[0]\n",
    "    if len(idx):\n",
    "        out.iloc[idx, out.columns.get_loc(\"qber\")] = np.clip(\n",
    "            out.iloc[idx][\"qber\"].to_numpy() + RNG.uniform(0.01, 0.04, len(idx)),\n",
    "            0.01, 0.15\n",
    "        )\n",
    "\n",
    "    # photon-number splitting\n",
    "    idx = np.where(kinds == \"pns\")[0]\n",
    "    if len(idx):\n",
    "        out.iloc[idx, out.columns.get_loc(\"decoy_signal_ratio\")] = np.clip(\n",
    "            RNG.normal(0.12, 0.06, len(idx)), 0.01, 0.40\n",
    "        )\n",
    "\n",
    "    # local oscillator manipulation\n",
    "    idx = np.where(kinds == \"lo_manipulation\")[0]\n",
    "    if len(idx):\n",
    "        out.iloc[idx, out.columns.get_loc(\"excess_noise\")] = np.clip(\n",
    "            RNG.normal(0.9, 0.25, len(idx)), 0.2, 2.0\n",
    "        )\n",
    "\n",
    "    return out\n",
    "\n",
    "# ---------- Add benign QKD to all rows ----------\n",
    "qkd = qkd_benign(len(df))\n",
    "df_base = pd.concat([df.reset_index(drop=True), qkd], axis=1)\n",
    "\n",
    "# ---------- Inject quantum_attack rows ----------\n",
    "quantum_frac = 0.10\n",
    "hybrid_frac  = 0.10\n",
    "\n",
    "n_q = int(len(df_base) * quantum_frac)\n",
    "n_h = int(len(df_base) * hybrid_frac)\n",
    "\n",
    "src_q = (\n",
    "    df_base[df_base[\"label\"] == \"benign\"]\n",
    "    .sample(n=min(n_q, (df_base[\"label\"] == \"benign\").sum()), random_state=42)\n",
    "    .reset_index(drop=True)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "k_q = RNG.choice(QUANTUM_ATTACKS, size=len(src_q))\n",
    "src_q[\"label\"] = \"quantum_attack\"\n",
    "src_q[\"attack_type\"] = k_q\n",
    "src_q[QCOLS] = apply_quantum_attack(src_q[QCOLS], k_q)\n",
    "\n",
    "# ---------- Inject hybrid_attack rows ----------\n",
    "src_h_pool = df_base[df_base[\"label\"] == \"classical_attack\"]\n",
    "\n",
    "src_h = (\n",
    "    src_h_pool\n",
    "    .sample(n=min(n_h, len(src_h_pool)), random_state=42)\n",
    "    .reset_index(drop=True)\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "k_h = RNG.choice(QUANTUM_ATTACKS, size=len(src_h))\n",
    "src_h[\"label\"] = \"hybrid_attack\"\n",
    "src_h[\"attack_type\"] = src_h[\"attack_type\"].astype(str) + \"+\" + k_h\n",
    "src_h[QCOLS] = apply_quantum_attack(src_h[QCOLS], k_h)\n",
    "\n",
    "# ---------- Final hybrid dataset ----------\n",
    "df_final = pd.concat([df_base, src_q, src_h], ignore_index=True)\n",
    "\n",
    "df_final[\"label\"].value_counts()\n"
   ],
   "id": "699c00ac9862a26a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/Users/constan/Desktop/data\")\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Outputs will be saved to:\", OUT_DIR)\n"
   ],
   "id": "c814576bccf264c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def add_rules(df):\n",
    "    out = df.copy()\n",
    "    out[\"rule_qber_high\"] = (out[\"qber\"] >= 0.06).astype(int)\n",
    "    out[\"rule_detector_imb_high\"] = (out[\"detector_imbalance\"] >= 0.20).astype(int)\n",
    "    out[\"rule_decoy_anom\"] = (out[\"decoy_signal_ratio\"].sub(0.30).abs() >= 0.12).astype(int)\n",
    "    out[\"rule_blind_suspect\"] = ((out[\"optical_power_rel\"] >= 1.30) & (out[\"qber\"] <= 0.05)).astype(int)\n",
    "    out[\"rule_score\"] = out[[\"rule_qber_high\",\"rule_detector_imb_high\",\"rule_decoy_anom\",\"rule_blind_suspect\"]].sum(axis=1)\n",
    "    return out\n",
    "\n",
    "df_final = add_rules(df_final)\n",
    "\n",
    "OUT_PARQUET = OUT_DIR / \"qa_ids_hybrid_with_rules.parquet\"\n",
    "df_final.to_parquet(OUT_PARQUET, index=False)\n",
    "\n",
    "print(\"Saved:\", OUT_PARQUET)\n",
    "print(df_final.columns.tolist())\n"
   ],
   "id": "946ffde791e23c97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "LABELS4 = [\"benign\",\"classical_attack\",\"quantum_attack\",\"hybrid_attack\"]\n",
    "\n",
    "rule_cols = [\"rule_qber_high\",\"rule_detector_imb_high\",\"rule_decoy_anom\",\"rule_blind_suspect\",\"rule_score\"]\n",
    "quantum_cols = QCOLS\n",
    "classical_cols = keep\n",
    "cat_cols = [c for c in [\"Protocol\"] if c in df_final.columns and c in classical_cols]\n",
    "\n",
    "# choose model: \"hgb\" (fast + strong) or \"logreg\" (very fast baseline)\n",
    "MODEL = \"hgb\"\n",
    "\n",
    "# downsample per class (keeps class balance, reproducible)\n",
    "N_PER_CLASS = 150_000  # 150k * 4 = 600k rows\n",
    "\n",
    "# -------------------------\n",
    "# Downsample (recommended)\n",
    "# -------------------------\n",
    "df_train = (\n",
    "    df_final.groupby(\"label\", group_keys=False)\n",
    "    .apply(lambda x: x.sample(n=min(N_PER_CLASS, len(x)), random_state=42))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Training set size:\", len(df_train))\n",
    "print(df_train[\"label\"].value_counts())\n",
    "\n",
    "# -------------------------\n",
    "# Model builder\n",
    "# -------------------------\n",
    "def make_model(name: str):\n",
    "    name = name.lower()\n",
    "    if name == \"hgb\":\n",
    "        # Works great on large tabular data\n",
    "        return HistGradientBoostingClassifier(\n",
    "            max_depth=None,\n",
    "            learning_rate=0.08,\n",
    "            max_iter=300,\n",
    "            random_state=42,\n",
    "        )\n",
    "    if name == \"logreg\":\n",
    "        return LogisticRegression(max_iter=3000, n_jobs=-1)\n",
    "    raise ValueError(\"MODEL must be 'hgb' or 'logreg'\")\n",
    "\n",
    "# -------------------------\n",
    "# Utility: run experiment\n",
    "# -------------------------\n",
    "def run_model(tag, feature_cols, cat_cols):\n",
    "    feature_cols = [c for c in feature_cols if c in df_train.columns]\n",
    "    cat_cols = [c for c in cat_cols if c in feature_cols]\n",
    "\n",
    "    X = df_train[feature_cols].copy()\n",
    "    y = df_train[\"label\"].copy()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    num_cols = [c for c in feature_cols if c not in cat_cols]\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", StandardScaler(), num_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    clf = make_model(MODEL)\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"clf\", clf)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    pred = pipe.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    report = classification_report(y_test, pred, labels=LABELS4, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, pred, labels=LABELS4)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"{tag} | model={MODEL} | accuracy={acc:.4f}\")\n",
    "    print(\"=\"*100)\n",
    "    print(report)\n",
    "\n",
    "    # Save report\n",
    "    (OUT_DIR / f\"{tag.replace(' ','_').lower()}_report.txt\").write_text(\n",
    "        f\"{tag} | model={MODEL} | accuracy={acc:.4f}\\n\\n{report}\\n\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.imshow(cm)\n",
    "    plt.title(f\"{tag} ({MODEL})\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(range(len(LABELS4)), LABELS4, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(len(LABELS4)), LABELS4)\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_path = OUT_DIR / f\"{tag.replace(' ','_').lower()}_confusion_matrix.png\"\n",
    "    plt.savefig(fig_path, dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Saved:\", fig_path)\n",
    "\n",
    "# -------------------------\n",
    "# Experiments (dissertation-style)\n",
    "# -------------------------\n",
    "\n",
    "# 1) Classical-only IDS baseline\n",
    "run_model(\"Classical-only IDS\", classical_cols, cat_cols)\n",
    "\n",
    "# 2) Quantum-only ML detector (QKD telemetry + rules)\n",
    "run_model(\"Quantum-only (QKD + rules)\", quantum_cols + rule_cols, [])\n",
    "\n",
    "# 3) Full QA-IDS (classical + quantum + rules)\n",
    "run_model(\"Full QA-IDS\", classical_cols + quantum_cols + rule_cols, cat_cols)\n",
    "\n",
    "print(\"\\nâœ… Done. Reports & plots saved in:\", OUT_DIR)"
   ],
   "id": "2e832cb0b474ca8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
